{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "# Parameters\n",
    "alpha_target = 0.05 # Desired Type I error\n",
    "beta_target = 0.2 # Desired Type II error (power = 0.8)\n",
    "mu_null = 0 # Mean under H0\n",
    "mu_alt = 0.5 # Mean under H1\n",
    "sigma = 1 # Std Deviation\n",
    "#n_sim = 10000 # Monte Carlo iterations\n",
    "n_sim = 500 # Monte Carlo iterations\n",
    "def simulate_type_errors(n):\n",
    "    # Simulate Type I Error\n",
    "    data_H0 = np.random.normal(mu_null, sigma, size=(n_sim, n))\n",
    "    pvals_H0 = [ttest_1samp(row, mu_null).pvalue for row in data_H0]\n",
    "    type_I_error = np.mean(np.array(pvals_H0) < alpha_target)\n",
    "\n",
    "    # Simulate Type II Error\n",
    "    data_H1 = np.random.normal(mu_alt, sigma, size=(n_sim, n))\n",
    "    pvals_H1 = [ttest_1samp(row, mu_null).pvalue for row in data_H1]\n",
    "    type_II_error = np.mean(np.array(pvals_H1) >= alpha_target)\n",
    "\n",
    "    return type_I_error, type_II_error\n",
    "\n",
    "# Search for the smallest n meeting both error constraints\n",
    "for n in range(2,200):\n",
    "    type_I, type_II = simulate_type_errors(n)\n",
    "    if type_I <= alpha_target and type_II <= beta_target:\n",
    "        print(f\"Minimum n = {n}\")\n",
    "        print(f\"Type I error = ~{type_I:.3f}, Type II error = ~{type_II:.3f}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Parameters\n",
    "alpha_target = 0.05 # Desired Type I error\n",
    "beta_target = 0.2 # Desired Type II error (power = 0.8)\n",
    "mu_null = 0 # Mean under H0\n",
    "mu_alt = 0.5 # Mean under H1\n",
    "sigma = 1 # Std Deviation\n",
    "#n_sim = 10000 # Monte Carlo iterations\n",
    "n_sim = 500 # Monte Carlo iterations\n",
    "\n",
    "def simulate_type_errors(n):\n",
    "    # H0: same mean\n",
    "    group1_H0 = np.random.normal(mu_null, sigma, size=(n_sim, n))\n",
    "    group2_H0 = np.random.normal(mu_null, sigma, size=(n_sim, n))\n",
    "    pvals_H0 = [ttest_ind(g1, g2, equal_var=True).pvalue \n",
    "                for g1, g2 in zip(group1_H0, group2_H0)]\n",
    "    type_I_error = np.mean(np.array(pvals_H0) < alpha_target)\n",
    "\n",
    "    # H1: different means\n",
    "    group1_H1 = np.random.normal(mu_null, sigma, size=(n_sim, n))\n",
    "    group2_H1 = np.random.normal(mu_alt, sigma, size=(n_sim, n))\n",
    "    pvals_H1 = [ttest_ind(g1, g2, equal_var=True).pvalue \n",
    "                for g1, g2 in zip(group1_H1, group2_H1)]\n",
    "    type_II_error = np.mean(np.array(pvals_H1) >= alpha_target)\n",
    "\n",
    "    return type_I_error, type_II_error\n",
    "\n",
    "# Search for the smallest n meeting both error constraints\n",
    "for n in range(2,200):\n",
    "    type_I, type_II = simulate_type_errors(n)\n",
    "    if type_I <= alpha_target and type_II <= beta_target:\n",
    "        print(f\"Minimum n = {n}\")\n",
    "        print(f\"Type I error = ~{type_I:.3f}, Type II error = ~{type_II:.3f}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters\n",
    "alpha_target = 0.05    # Desired Type I error\n",
    "beta_target = 0.2      # Desired Type II error (power = 0.8)\n",
    "n_sim = 500         # Monte Carlo iterations\n",
    "\n",
    "def simulate_type_errors(n):\n",
    "    \"\"\"\n",
    "    Simulate Type I and Type II error rates for a chi-square test of independence\n",
    "    on a 2x2 table, with n samples per group.\n",
    "    \"\"\"\n",
    "    # ----- Type I Error -----\n",
    "    pvals_H0 = []\n",
    "    for _ in range(n_sim):\n",
    "        # Under H0: both groups have the same probability distribution\n",
    "        group1 = np.random.binomial(1, 0.5, n)  # binary outcome (0 or 1)\n",
    "        group2 = np.random.binomial(1, 0.5, n)\n",
    "        \n",
    "        # Build contingency table\n",
    "        table = np.array([\n",
    "            [np.sum(group1 == 0), np.sum(group1 == 1)],\n",
    "            [np.sum(group2 == 0), np.sum(group2 == 1)]\n",
    "        ], dtype=float) # convert to ifloat so we can add 0.5\n",
    "\n",
    "        table += 0.5 # continuity correction to avoid zeros\n",
    "        \n",
    "        _, p, _, _ = chi2_contingency(table, correction=False)\n",
    "        pvals_H0.append(p)\n",
    "    \n",
    "    type_I_error = np.mean(np.array(pvals_H0) < alpha_target)\n",
    "\n",
    "    # ----- Type II Error -----\n",
    "    pvals_H1 = []\n",
    "    for _ in range(n_sim):\n",
    "        # Under H1: groups have different distributions\n",
    "        group1 = np.random.binomial(1, 0.5, n)   # 50% success rate\n",
    "        group2 = np.random.binomial(1, 0.7, n)   # 70% success rate (shifted)\n",
    "        \n",
    "        table = np.array([\n",
    "            [np.sum(group1 == 0), np.sum(group1 == 1)],\n",
    "            [np.sum(group2 == 0), np.sum(group2 == 1)]\n",
    "        ], dtype=float) # convert to float so we can add 0.5\n",
    "\n",
    "        table += 0.5 # continuity correction to avoid zeros\n",
    "        \n",
    "        _, p, _, _ = chi2_contingency(table, correction=False)\n",
    "        pvals_H1.append(p)\n",
    "    \n",
    "    type_II_error = np.mean(np.array(pvals_H1) >= alpha_target)\n",
    "\n",
    "    return type_I_error, type_II_error\n",
    "\n",
    "# Search for minimum sample size\n",
    "for n in range(2,200):  # start at 5 because chi-square needs >0 in each cell\n",
    "    type_I, type_II = simulate_type_errors(n)\n",
    "    if type_I <= alpha_target and type_II <= beta_target:\n",
    "        print(f\"Minimum n per group = {n}\")\n",
    "        print(f\"Type I error ≈ {type_I:.3f}, Type II error ≈ {type_II:.3f}\")\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters\n",
    "alpha_target = 0.05\n",
    "beta_target = 0.2\n",
    "n_sim = 2000   # fewer sims for speed during plotting\n",
    "\n",
    "def simulate_type_errors(n):\n",
    "    \"\"\"Return Type I and Type II error rates for chi-square test.\"\"\"\n",
    "    pvals_H0 = []\n",
    "    for _ in range(n_sim):\n",
    "        g1 = np.random.binomial(1, 0.5, n)\n",
    "        g2 = np.random.binomial(1, 0.5, n)\n",
    "        table = np.array([\n",
    "            [np.sum(g1 == 0), np.sum(g1 == 1)],\n",
    "            [np.sum(g2 == 0), np.sum(g2 == 1)]\n",
    "        ], dtype=float)\n",
    "        table += 0.5  # avoid zero counts\n",
    "        _, p, _, _ = chi2_contingency(table, correction=False)\n",
    "        pvals_H0.append(p)\n",
    "    type_I = np.mean(np.array(pvals_H0) < alpha_target)\n",
    "\n",
    "    pvals_H1 = []\n",
    "    for _ in range(n_sim):\n",
    "        g1 = np.random.binomial(1, 0.5, n)\n",
    "        g2 = np.random.binomial(1, 0.7, n)  # shifted probability\n",
    "        table = np.array([\n",
    "            [np.sum(g1 == 0), np.sum(g1 == 1)],\n",
    "            [np.sum(g2 == 0), np.sum(g2 == 1)]\n",
    "        ], dtype=float)\n",
    "        table += 0.5\n",
    "        _, p, _, _ = chi2_contingency(table, correction=False)\n",
    "        pvals_H1.append(p)\n",
    "    type_II = np.mean(np.array(pvals_H1) >= alpha_target)\n",
    "\n",
    "    return type_I, type_II\n",
    "\n",
    "# Store results for plotting\n",
    "ns = range(5, 100)\n",
    "typeI_errors = []\n",
    "typeII_errors = []\n",
    "\n",
    "for n in ns:\n",
    "    type_I, type_II = simulate_type_errors(n)\n",
    "    typeI_errors.append(type_I)\n",
    "    typeII_errors.append(type_II)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(ns, typeI_errors, label='Type I Error', color='red')\n",
    "plt.plot(ns, typeII_errors, label='Type II Error', color='blue')\n",
    "plt.axhline(alpha_target, color='red', linestyle='--', label='Target α')\n",
    "plt.axhline(beta_target, color='blue', linestyle='--', label='Target β')\n",
    "plt.xlabel(\"Sample size per group (n)\")\n",
    "plt.ylabel(\"Error rate\")\n",
    "plt.title(\"Monte Carlo Error Rates vs Sample Size\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataAnalysis_Statistics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
